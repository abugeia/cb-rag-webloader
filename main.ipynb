{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mollama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OllamaEmbeddings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "from langchain.schema import get_document_prompt\n",
    "from langgraph import LLGraph\n",
    "from langgraph.utilities import is_template\n",
    "from openwebui import openwebui  # Assuming openwebui is installed and accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = 'nomic-embed-text'\n",
    "CHAT_MODEL = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_documentation_site(base_url):\n",
    "    # Step 1: Fetch the main documentation page\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    all_text_content = []\n",
    "\n",
    "    # Step 2: Extract all relevant links (e.g., to individual sections)\n",
    "    # Assume they are contained in <a> tags with a specific class or ID\n",
    "    documentation_links = soup.find_all('a', href=True)  # Adjust selector as needed\n",
    "\n",
    "    for link in documentation_links:\n",
    "        section_url = link['href']\n",
    "        \n",
    "        # Make sure the URL is complete (handle relative URLs)\n",
    "        if not section_url.startswith('http'):\n",
    "            section_url = requests.compat.urljoin(base_url, section_url)\n",
    "        \n",
    "        # Step 3: Visit each page and collect content\n",
    "        section_response = requests.get(section_url)\n",
    "        section_soup = BeautifulSoup(section_response.text, 'html.parser')\n",
    "        \n",
    "        # Extract text from the current documentation page\n",
    "        all_text_content.append(section_soup.get_text())\n",
    "\n",
    "    return \"\\n\".join(all_text_content)\n",
    "\n",
    "# Example usage:\n",
    "base_documentation_url = 'https://example.com/documentation'\n",
    "documentation_content = scrape_documentation_site(base_documentation_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------- Configuration -----------------------\n",
    "\n",
    "# Ollama Model\n",
    "OLLAMA_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\" # or your preferred Ollama model\n",
    "\n",
    "# Vectorstore Directory\n",
    "VECTORDB_DIR = \"web_data\"  # Where ChromaDB will store the embeddings\n",
    "\n",
    "# Webpage URL to scrape\n",
    "WEBSITE_URL = \"https://www.example.com\"  # Replace with the URL you want to scrape\n",
    "\n",
    "\n",
    "# ----------------------- Functions -----------------------\n",
    "\n",
    "def scrape_website(url):\n",
    "    \"\"\"\n",
    "    Scrapes a website and returns a list of documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = WebBaseLoader(url)\n",
    "        documents = loader.load()\n",
    "        print(f\"Successfully scraped {url}\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Splits documents into chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "def create_vectorstore(documents):\n",
    "    \"\"\"\n",
    "    Creates a Chroma vectorstore from a list of documents.\n",
    "    \"\"\"\n",
    "    embeddings = OllamaEmbeddings(model=OLLAMA_MODEL)\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents, embedding=embeddings, persist_directory=VECTORDB_DIR\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def create_retrieval_qa_chain(vectorstore, llm):\n",
    "    \"\"\"\n",
    "    Creates a RetrievalQA chain.\n",
    "    \"\"\"\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever()\n",
    "    )\n",
    "\n",
    "\n",
    "def create_langgraph(retrieval_qa_chain, llm):\n",
    "    \"\"\"\n",
    "    Creates a Langgraph graph from a RetrievalQA chain.\n",
    "    \"\"\"\n",
    "    # Define the Langgraph graph\n",
    "    graph = LLGraph(\n",
    "        llm=llm,\n",
    "        get_prompt=get_document_prompt,  # Use Langgraph's get_document_prompt\n",
    "        prompt_template_string=retrieval_qa_chain.retriever.knowledge_graph_prompt_template,  # Use Retriever's prompt template\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "\n",
    "def initialize_chatbot():\n",
    "    \"\"\"\n",
    "    Initializes the chatbot process: scrapes, creates vectorstore, and sets up the Langgraph.\n",
    "    \"\"\"\n",
    "    # 1. Scrape the website\n",
    "    documents = scrape_website(WEBSITE_URL)\n",
    "\n",
    "    if not documents:\n",
    "        print(\"No documents scraped.  Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 2. Chunk the documents\n",
    "    chunked_documents = chunk_documents(documents)\n",
    "\n",
    "    # 3. Create/Update the vectorstore\n",
    "    if os.path.exists(VECTORDB_DIR):\n",
    "        print(\"Vectorstore already exists.  Loading...\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=VECTORDB_DIR, embedding=OllamaEmbeddings(model=OLLAMA_MODEL)\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating new vectorstore...\")\n",
    "        vectorstore = create_vectorstore(chunked_documents)\n",
    "\n",
    "    # 4. Initialize Ollama LLM\n",
    "    llm = Ollama(model=OLLAMA_MODEL)\n",
    "\n",
    "    # 5. Create RetrievalQA Chain\n",
    "    retrieval_qa_chain = create_retrieval_qa_chain(vectorstore, llm)\n",
    "\n",
    "    # 6. Create Langgraph\n",
    "    langgraph = create_langgraph(retrieval_qa_chain, llm)\n",
    "\n",
    "    return langgraph, llm, vectorstore\n",
    "\n",
    "\n",
    "# ----------------------- Main Execution -----------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        langgraph, llm, vectorstore = initialize_chatbot()\n",
    "\n",
    "        print(\"Chatbot initialized. Ready to use with OpenWebUI.\")\n",
    "\n",
    "        # Optional: Print some info\n",
    "        print(f\"Using Ollama model: {OLLAMA_MODEL}\")\n",
    "        print(f\"Vectorstore directory: {VECTORDB_DIR}\")\n",
    "\n",
    "        # Register the Langgraph with OpenWebUI (assuming it's integrated)\n",
    "        # Replace 'my_langgraph' with the name you want to give it in OpenWebUI\n",
    "        openwebui.register_langgraph(langgraph, \"web_chatbot\") # Replace 'web_chatbot' with a suitable name.\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
